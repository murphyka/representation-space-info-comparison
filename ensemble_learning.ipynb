{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUHm2okAuA8m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "tfkl = tf.keras.layers\n",
        "import matplotlib\n",
        "\n",
        "import utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bhat_mats = []\n",
        "ensemble_sizes = [2, 3, 5, 10, 15, 20, 30, 50]\n",
        "number_repeat_training_runs = 5  ## the number of times to repeat the experiment for statistics\n",
        "\n",
        "## \"Standard candle\" is the codename for the set of data points with which to compute the Bhattacharyya matrices\n",
        "standard_candle_size = 200\n",
        "pts_standard_candle = np.linspace(0, 2*np.pi,standard_candle_size, endpoint=False)\n",
        "pts_standard_candle = np.stack([np.cos(pts_standard_candle), np.sin(pts_standard_candle)], -1)\n",
        "\n",
        "number_training_steps = 3000\n",
        "batch_size = 2048\n",
        "beta = 3e-2\n",
        "learning_rate = 1e-3\n",
        "data_dimensionality = 2\n",
        "enc_arch_spec = [256]*2\n",
        "dec_arch_spec = [256]*2\n",
        "activation_fn = 'tanh'\n",
        "number_bottleneck_channels = 1\n",
        "\n",
        "for trial in range(number_repeat_training_runs*np.max(ensemble_sizes)):\n",
        "  encoder = tf.keras.Sequential([tf.keras.Input((data_dimensionality,))] + \\\n",
        "                                [tfkl.Dense(number_units, activation_fn) for number_units in enc_arch_spec] + \\\n",
        "                                [tfkl.Dense(2*number_bottleneck_channels)])\n",
        "  decoder = tf.keras.Sequential([tf.keras.Input((number_bottleneck_channels,))] + \\\n",
        "                                [tfkl.Dense(number_units, activation_fn) for number_units in dec_arch_spec] + \\\n",
        "                                [tfkl.Dense(data_dimensionality)])\n",
        "  all_trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "  mse = tf.keras.losses.MeanSquaredError()\n",
        "  @tf.function\n",
        "  def train_step(pts):\n",
        "    with tf.GradientTape() as tape:\n",
        "      embs_mus, embs_logvars = tf.split(encoder(pts), 2, axis=-1)\n",
        "      kl = tf.reduce_mean(tf.reduce_sum(0.5 * (tf.square(embs_mus) + tf.exp(embs_logvars) - embs_logvars - 1.), axis=-1), axis=0)\n",
        "      reparameterized_embs = tf.random.normal(embs_mus.shape, mean=embs_mus, stddev=tf.exp(embs_logvars/2.))\n",
        "      recon = decoder(reparameterized_embs)\n",
        "      reconstruction_loss = mse(pts, recon)\n",
        "      loss = tf.reduce_mean(reconstruction_loss) + beta * kl\n",
        "    grads = tape.gradient(loss, all_trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, all_trainable_variables))\n",
        "    return reconstruction_loss, kl\n",
        "\n",
        "  recon_loss_series, kl_loss_series = [[], []]\n",
        "  for step in range(number_training_steps):\n",
        "    batch_theta = np.random.uniform(0, 2*np.pi, size=batch_size)\n",
        "    batch_pts = np.stack([np.cos(batch_theta), np.sin(batch_theta)], -1)\n",
        "    recon_loss, kl = train_step(batch_pts)\n",
        "    recon_loss_series.append(recon_loss.numpy())\n",
        "    kl_loss_series.append(kl.numpy())\n",
        "\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(recon_loss_series, 'k', lw=2)\n",
        "  plt.ylabel('Recon', fontsize=15)\n",
        "  plt.xlabel('Step', fontsize=15)\n",
        "  plt.gca().twinx().plot(kl_loss_series, lw=2)\n",
        "  plt.ylabel('KL', color='b', fontsize=15)\n",
        "  plt.show()\n",
        "\n",
        "  standard_candle_embs = encoder(pts_standard_candle)\n",
        "  mus, logvars = tf.split(standard_candle_embs, 2, -1)\n",
        "  bhat_mats.append(utils.bhattacharyya_dist_mat(mus, logvars))\n",
        "\n",
        "  print('Computed bhat mat, onto the next run.')"
      ],
      "metadata": {
        "id": "zSlp-6BVuLyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def compute_vi_similarity(bhat1, bhat2):\n",
        "  i11 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat1*2, axis=1))\n",
        "  i22 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat2*2, axis=1))\n",
        "  i12 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat1-bhat2, axis=1))\n",
        "  return tf.exp(-(i12*2 - i11 - i22))\n",
        "\n",
        "@tf.function\n",
        "def compute_nmi(bhat1, bhat2):\n",
        "  i1 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat1, axis=1))\n",
        "  i2 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat2, axis=1))\n",
        "  i11 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat1*2, axis=1))\n",
        "  i22 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat2*2, axis=1))\n",
        "  i12 = -tf.reduce_mean(tf.math.reduce_logsumexp(-bhat1-bhat2, axis=1))\n",
        "  return (i1+i2-i12) / tf.sqrt((2*i1-i11)*(2*i2-i22))"
      ],
      "metadata": {
        "id": "MVFJb3yzvA4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uniformity_metric(mus):\n",
        "  adj_vecs = mus - np.roll(mus, 1, axis=0)\n",
        "  distances = np.linalg.norm(adj_vecs, axis=-1, ord=2)\n",
        "  return np.sqrt(np.mean(distances**2)) / np.mean(distances)"
      ],
      "metadata": {
        "id": "FOqDhvn5vTa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_emb_dim = 2\n",
        "number_opt_steps = 20000\n",
        "uniformity_metric_values = []\n",
        "learning_rate = 3e0\n",
        "for sim_method_name, sim_method in zip(['vi', 'nmi'], [compute_vi_similarity, compute_nmi]):\n",
        "  for ensemble_size in ensemble_sizes:\n",
        "    for repeat_ind in range(number_repeat_training_runs):\n",
        "      mus_var = tf.Variable(tf.random.normal((standard_candle_size, re_emb_dim), stddev=0.05), trainable=True)\n",
        "      logvars_var = tf.Variable(tf.zeros((standard_candle_size, re_emb_dim)), trainable=True)\n",
        "      all_trainable_variables = [mus_var, logvars_var]\n",
        "      bhats_to_use = bhat_mats[repeat_ind*np.max(ensemble_sizes):repeat_ind*np.max(ensemble_sizes)+ensemble_size]\n",
        "      optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "      @tf.function\n",
        "      def compute_sum_dist():\n",
        "        with tf.GradientTape() as tape:\n",
        "          bhat_mat_opt = utils.bhattacharyya_dist_mat_tf(mus_var, logvars_var)\n",
        "          total_dist = 0.\n",
        "          for other_bhat in bhats_to_use:\n",
        "            dist = sim_method(bhat_mat_opt, tf.cast(other_bhat, tf.float32))\n",
        "            total_dist -= dist\n",
        "          loss = total_dist / ensemble_size\n",
        "        grads = tape.gradient(loss, all_trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, all_trainable_variables))\n",
        "        return total_dist\n",
        "      dist_series = []\n",
        "      for opt_step in range(number_opt_steps):\n",
        "        total_dist = compute_sum_dist()\n",
        "        dist_series.append(total_dist)\n",
        "        if (opt_step % 1000) == 0:\n",
        "          bhat_mat_opt = utils.bhattacharyya_dist_mat_tf(mus_var, logvars_var)\n",
        "          plt.figure(figsize=(10, 5))\n",
        "          plt.subplot(121)\n",
        "          plt.imshow(tf.exp(-bhat_mat_opt), vmin=0, vmax=1)\n",
        "          plt.axis('off')\n",
        "          plt.subplot(122)\n",
        "          plt.scatter(*np.float32(mus_var.numpy()).T, c=np.linspace(0, 1, standard_candle_size), cmap='hsv')\n",
        "          plt.show()\n",
        "      plt.figure(figsize=(7, 4))\n",
        "      plt.plot(dist_series)\n",
        "      plt.ylabel('Distance', fontsize=15)\n",
        "      plt.xlabel('Step', fontsize=15)\n",
        "      plt.show()\n",
        "\n",
        "      bhat_mat_opt = utils.bhattacharyya_dist_mat_tf(mus_var, logvars_var)\n",
        "      plt.figure(figsize=(10, 5))\n",
        "      plt.subplot(121)\n",
        "      plt.imshow(tf.exp(-bhat_mat_opt), vmin=0, vmax=1, cmap='Blues_r')\n",
        "      plt.axis('off')\n",
        "      plt.subplot(122)\n",
        "      plt.scatter(*np.float32(mus_var.numpy()).T, c=np.linspace(0, 1, standard_candle_size), cmap='hsv')\n",
        "      plt.xticks([]); plt.yticks([])\n",
        "      plt.show()\n",
        "\n",
        "      uniformity_metric_values.append(uniformity_metric(np.float32(mus_var.numpy())))\n",
        "\n"
      ],
      "metadata": {
        "id": "oAMjtMylvJ2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 6))\n",
        "uniformity_metric_values = np.reshape(uniformity_metric_values, [2, -1, number_repeat_training_runs])\n",
        "for method_ind, sim_method_name in enumerate(['vi', 'nmi']):\n",
        "  plt.errorbar(ensemble_sizes, np.mean(uniformity_metric_values[method_ind], -1)-1., yerr=np.std(uniformity_metric_values[method_ind], -1),\n",
        "               ls='-', marker='os'[method_ind], markersize=12, lw=4, label=sim_method_name)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xticks(ensemble_sizes, ensemble_sizes)\n",
        "plt.xlabel('Ensemble size', fontsize=15)\n",
        "plt.ylabel('Uniformity metric-1', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e3ykFiNevx51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdgnCYLFpLIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}