{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyjLuSAE5Vjx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "tfkl = tf.keras.layers\n",
        "import os, time\n",
        "\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import scipy.ndimage as nim\n",
        "\n",
        "import scipy.stats\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "from netrep.metrics import GaussianStochasticMetric\n",
        "\n",
        "default_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "roman_numerals = ['i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'x']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "dFwoI7ZKhQbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Monte Carlo information evaluation\n",
        "def monte_carlo_info(mus, logvars, number_random_samples=10):\n",
        "  sample_size = 2000\n",
        "  chunk_eval_size = 10_000\n",
        "  info_estimates = []\n",
        "  emb_dim = mus.shape[-1]\n",
        "  for rand_sample in range(number_random_samples):\n",
        "    rand_inds = np.random.choice(mus.shape[0], size=sample_size)\n",
        "    rand_sample = tf.random.normal(shape=(sample_size, emb_dim),\n",
        "                                  mean=mus[rand_inds],\n",
        "                                  stddev=tf.exp(logvars[rand_inds]/2.),\n",
        "                                   dtype=tf.float64)\n",
        "    # rand_sample = tf.cast(rand_sample, tf.float64)\n",
        "    posterior_probs = compute_likelihoods(rand_sample, mus[rand_inds], logvars[rand_inds], diag=True)\n",
        "    marginal_probs = np.zeros((sample_size))\n",
        "    for start_ind in range(0, mus.shape[0], chunk_eval_size):\n",
        "      end_ind = min(start_ind+chunk_eval_size, mus.shape[0])\n",
        "      marginal_probs = marginal_probs + compute_likelihoods(rand_sample, mus[start_ind:end_ind], logvars[start_ind:end_ind])\n",
        "    marginal_probs = marginal_probs / mus.shape[0]\n",
        "\n",
        "    info_estimates.append(tf.math.log(posterior_probs/marginal_probs))\n",
        "  return np.mean(info_estimates)/np.log(2)\n",
        "\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def compute_likelihoods(samples, mus, logvars, diag=False):\n",
        "  mus = tf.cast(mus, tf.float64)\n",
        "  logvars = tf.cast(logvars, tf.float64)\n",
        "  sample_size = tf.shape(samples)[0]\n",
        "  evaluation_batch_size = tf.shape(mus)[0]\n",
        "  embedding_dimension = tf.shape(mus)[-1]\n",
        "  stddevs = tf.exp(logvars/2.)\n",
        "  # Expand dimensions to broadcast and compute the pairwise distances between\n",
        "  # the sampled points and the centers of the conditional distributions\n",
        "  samples = tf.reshape(samples,\n",
        "    [sample_size, 1, embedding_dimension])\n",
        "  mus = tf.reshape(mus, [1, evaluation_batch_size, embedding_dimension])\n",
        "  distances_ui_muj = samples - mus\n",
        "\n",
        "  normalized_distances_ui_muj = distances_ui_muj / tf.reshape(stddevs, [1, evaluation_batch_size, embedding_dimension])\n",
        "  p_ui_cond_xj = tf.exp(-tf.reduce_sum(normalized_distances_ui_muj**2, axis=-1)/2. - \\\n",
        "    tf.reshape(tf.reduce_sum(logvars, axis=-1), [1, evaluation_batch_size])/2.)\n",
        "  normalization_factor = (2.*np.pi)**(tf.cast(embedding_dimension, tf.float64)/2.)\n",
        "  p_ui_cond_xj = p_ui_cond_xj / normalization_factor\n",
        "  if diag:\n",
        "    return tf.linalg.diag_part(p_ui_cond_xj)\n",
        "  else:\n",
        "    return tf.reduce_sum(p_ui_cond_xj, axis=-1)"
      ],
      "metadata": {
        "id": "J-uCw4DR5u8c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Bhattacharyya-based information evaluation\n",
        "def bhattacharyya_dist_mat(mus, logvars):\n",
        "  \"\"\"Computes Bhattacharyya distances between multivariate Gaussians.\n",
        "  The Bhattacharyya coefficient is the exponentiated negative distance.\n",
        "  Args:\n",
        "    mus: [N, d] float array of the means of the Gaussians.\n",
        "    logvars: [N, d] float array of the log variances of the Gaussians (so we're assuming diagonal\n",
        "    covariance matrices; these are the logs of the diagonal).\n",
        "  Returns:\n",
        "    [N, N] array of distances.\n",
        "  \"\"\"\n",
        "  N = mus.shape[0]\n",
        "  embedding_dimension = mus.shape[1]\n",
        "\n",
        "  ## Manually broadcast\n",
        "  mus1 = np.tile(mus[:, np.newaxis], [1, N, 1])\n",
        "  logvars1 = np.tile(logvars[:, np.newaxis], [1, N, 1])\n",
        "  mus2 = np.tile(mus[np.newaxis], [N, 1, 1])\n",
        "  logvars2 = np.tile(logvars[np.newaxis], [N, 1, 1])\n",
        "  difference_mus = mus1 - mus2  # [N, M, embedding_dimension]; we want [N, N, embedding_dimension, 1]\n",
        "  difference_mus = difference_mus[..., np.newaxis]\n",
        "  difference_mus_T = np.transpose(difference_mus, [0, 1, 3, 2])\n",
        "\n",
        "  sigma_diag = 0.5 * (np.exp(logvars1) + np.exp(logvars2))  ## [N, N, embedding_dimension], but we want a diag mat [N, N, embedding_dimension, embedding_dimension]\n",
        "  sigma_mat = np.expand_dims(sigma_diag, -1) * np.expand_dims(np.ones_like(sigma_diag), -2) * np.reshape(np.eye(embedding_dimension), [1, 1, embedding_dimension, embedding_dimension])\n",
        "  sigma_mat_inv = np.expand_dims(1./sigma_diag, -1) * np.expand_dims(np.ones_like(sigma_diag), -2) * np.reshape(np.eye(embedding_dimension), [1, 1, embedding_dimension, embedding_dimension])\n",
        "\n",
        "  log_determinant_sigma = np.sum(np.log(sigma_diag), axis=-1)\n",
        "  log_determinant_sigma1 = np.sum(logvars1, axis=-1)\n",
        "  log_determinant_sigma2 = np.sum(logvars2, axis=-1)\n",
        "  term1 = 0.125 * (difference_mus_T @ sigma_mat_inv @ difference_mus).reshape([N, N])\n",
        "  term2 = 0.5 * (log_determinant_sigma - 0.5 * (log_determinant_sigma1  + log_determinant_sigma2))\n",
        "  return term1+term2\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def bhattacharyya_dist_mat_tf(mus, logvars):\n",
        "  \"\"\"Computes Bhattacharyya distances between multivariate Gaussians.\n",
        "  Args:\n",
        "    mus1: [N, d] float array of the means of the Gaussians.\n",
        "    logvars1: [N, d] float array of the log variances of the Gaussians (so we're assuming diagonal\n",
        "    covariance matrices; these are the logs of the diagonal).\n",
        "  Returns:\n",
        "    [N, M] array of distances.\n",
        "  \"\"\"\n",
        "  N = tf.shape(mus)[0]\n",
        "  embedding_dimension = tf.shape(mus)[1]\n",
        "\n",
        "  mus = tf.cast(mus, tf.float64)\n",
        "  logvars = tf.cast(logvars, tf.float64)\n",
        "\n",
        "  ## Manually broadcast in case either M or N is 1\n",
        "  mus1 = tf.tile(tf.expand_dims(mus, 1), [1, N, 1])\n",
        "  logvars1 = tf.tile(tf.expand_dims(logvars, 1), [1, N, 1])\n",
        "  mus2 = tf.tile(tf.expand_dims(mus, 0), [N, 1, 1])\n",
        "  logvars2 = tf.tile(tf.expand_dims(logvars, 0), [N, 1, 1])\n",
        "  difference_mus = mus1 - mus2  # [N, M, embedding_dimension]; we want [N, M, embedding_dimension, 1]\n",
        "  difference_mus = tf.expand_dims(difference_mus, -1)\n",
        "  difference_mus_T = tf.transpose(difference_mus, [0, 1, 3, 2])\n",
        "\n",
        "  sigma_diag = 0.5 * (tf.exp(logvars1) + tf.exp(logvars2))  ## [N, M, embedding_dimension], but we want a diag mat [N, M, embedding_dimension, embedding_dimension]\n",
        "  # sigma_mat = np.apply_along_axis(np.diag, -1, sigma_diag)\n",
        "  sigma_mat = tf.expand_dims(sigma_diag, -1) * tf.expand_dims(tf.ones_like(sigma_diag, dtype=tf.float64), -2) * tf.reshape(tf.eye(embedding_dimension, dtype=tf.float64), [1, 1, embedding_dimension, embedding_dimension])\n",
        "  # sigma_mat_inv = np.apply_along_axis(np.diag, -1, 1./sigma_diag)\n",
        "  sigma_mat_inv = tf.expand_dims(1./sigma_diag, -1) * tf.expand_dims(tf.ones_like(sigma_diag, dtype=tf.float64), -2) * tf.reshape(tf.eye(embedding_dimension, dtype=tf.float64), [1, 1, embedding_dimension, embedding_dimension])\n",
        "\n",
        "  log_determinant_sigma = tf.reduce_sum(tf.math.log(sigma_diag), axis=-1)\n",
        "  log_determinant_sigma1 = tf.reduce_sum(logvars1, axis=-1)\n",
        "  log_determinant_sigma2 = tf.reduce_sum(logvars2, axis=-1)\n",
        "  term1 = 0.125 * tf.reshape(difference_mus_T @ sigma_mat_inv @ difference_mus, [N, N])\n",
        "  term2 = 0.5 * (log_determinant_sigma - 0.5 * (log_determinant_sigma1 + log_determinant_sigma2))\n",
        "  return term1+term2\n",
        "\n",
        "@tf.function(experimental_relax_shapes=True)\n",
        "def bhat_info_tf(mus, logvars):\n",
        "  bhat_dist_mat = bhattacharyya_dist_mat_tf(mus, logvars)\n",
        "  info = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat_dist_mat))))\n",
        "  return info\n",
        "\n",
        "@tf.function\n",
        "def compute_nmi_bhat_tf(bhat1, bhat2):\n",
        "  i1 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1), axis=1)))\n",
        "  i2 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat2), axis=1)))\n",
        "  i11 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1*2), axis=1)))\n",
        "  i22 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat2*2), axis=1)))\n",
        "  i12 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1-bhat2), axis=1)))\n",
        "  return (i1+i2-i12) / tf.sqrt((2*i1-i11)*(2*i2-i22))\n",
        "\n",
        "@tf.function\n",
        "def compute_vi_bhat_tf(bhat1, bhat2):\n",
        "  i11 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1*2), axis=1)))\n",
        "  i22 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat2*2), axis=1)))\n",
        "  i12 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1-bhat2), axis=1)))\n",
        "  return 2*i12 - i11 - i22"
      ],
      "metadata": {
        "id": "5jKIRrgudIXV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute stochastic shape metric with mus and logvars\n",
        "def compute_stoch_shape_metric(mus1, logvars1, mus2, logvars2):\n",
        "  ct = time.time()\n",
        "  alpha = 1  ## Wasserstein comparing means and covariances\n",
        "  metric = GaussianStochasticMetric(alpha, init='rand', n_restarts=50)\n",
        "\n",
        "  covs1 = np.apply_along_axis(np.diag, 1, np.exp(-logvars1))\n",
        "  covs2 = np.apply_along_axis(np.diag, 1, np.exp(-logvars2))\n",
        "\n",
        "  X1 = (mus1, covs1)\n",
        "  X2 = (mus2, covs2)\n",
        "\n",
        "  metric.fit(X1, X2)\n",
        "\n",
        "  score = metric.score(X1, X2)\n",
        "  return score"
      ],
      "metadata": {
        "id": "9LKBoQ5TF3Gs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CKA\n",
        "fingerprint_size = N = 64\n",
        "\n",
        "centering_matrix = np.eye(fingerprint_size) - np.ones((fingerprint_size, fingerprint_size))/fingerprint_size\n",
        "def compute_cka(arr1, arr2):\n",
        "  sim11 = np.trace(arr1 @ centering_matrix @ arr1 @ centering_matrix)\n",
        "  sim22 = np.trace(arr2 @ centering_matrix @ arr2 @ centering_matrix)\n",
        "  sim12 = np.trace(arr1 @ centering_matrix @ arr2 @ centering_matrix)\n",
        "  cka = sim12 / np.sqrt(sim11*sim22)\n",
        "  return cka\n",
        "\n",
        "@tf.function\n",
        "def compute_cka_tf(arr1, arr2):\n",
        "  sim11 = tf.linalg.trace(arr1 @ centering_matrix @ arr1 @ centering_matrix)\n",
        "  sim22 = tf.linalg.trace(arr2 @ centering_matrix @ arr2 @ centering_matrix)\n",
        "  sim12 = tf.linalg.trace(arr1 @ centering_matrix @ arr2 @ centering_matrix)\n",
        "  cka = sim12 / tf.sqrt(sim11*sim22)\n",
        "  return cka\n",
        "\n",
        "@tf.function\n",
        "def pairwise_l2_distance(pts1, pts2):\n",
        "  \"\"\"Computes squared L2 distances between each element of each set of points.\n",
        "  Args:\n",
        "    pts1: [N, d] tensor of points.\n",
        "    pts2: [M, d] tensor of points.\n",
        "  Returns:\n",
        "    distance_matrix: [N, M] tensor of distances.\n",
        "  \"\"\"\n",
        "  norm1 = tf.reduce_sum(tf.square(pts1), axis=-1, keepdims=True)\n",
        "  norm2 = tf.reduce_sum(tf.square(pts2), axis=-1)\n",
        "  norm2 = tf.expand_dims(norm2, -2)\n",
        "  distance_matrix = tf.maximum(\n",
        "      norm1 + norm2 - 2.0 * tf.matmul(pts1, pts2, transpose_b=True), 0.0)\n",
        "  return distance_matrix"
      ],
      "metadata": {
        "id": "aoAXkv3EG_m8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NMI and VI through Bhattacharyya matrices\n",
        "\n",
        "@tf.function\n",
        "def compute_nmi_bhat_tf(bhat1, bhat2):\n",
        "  i1 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1), axis=1)))\n",
        "  i2 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat2), axis=1)))\n",
        "  i11 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1*2), axis=1)))\n",
        "  i22 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat2*2), axis=1)))\n",
        "  i12 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1-bhat2), axis=1)))\n",
        "  return (i1+i2-i12) / tf.sqrt((2*i1-i11)*(2*i2-i22))\n",
        "\n",
        "@tf.function\n",
        "def compute_vi_bhat_tf(bhat1, bhat2):\n",
        "  i11 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1*2), axis=1)))\n",
        "  i22 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat2*2), axis=1)))\n",
        "  i12 = -tf.reduce_mean(tf.math.log(tf.reduce_mean(tf.exp(-bhat1-bhat2), axis=1)))\n",
        "  return 2*i12 - i11 - i22"
      ],
      "metadata": {
        "id": "rT-gZko6sNDQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep work"
      ],
      "metadata": {
        "id": "1ZTofX9mhTG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate the nine synthetic representation spaces, embedding the points x=1...64\n",
        "sqrt_N = 8\n",
        "N = sqrt_N**2\n",
        "\n",
        "cmap = plt.get_cmap('viridis')\n",
        "alpha = 0.5\n",
        "\n",
        "discrete_jitter_level = 0.\n",
        "\n",
        "x = np.arange(N)\n",
        "\n",
        "u_mus_all, u_logvars_all = [[], []]\n",
        "\n",
        "constant_variance_offset = 0.1\n",
        "\n",
        "###### Spiral: constant variance\n",
        "\n",
        "spiral_freq = 0.2\n",
        "u_mus = np.sqrt(x).reshape([-1, 1])*np.stack([np.cos(2*np.pi*np.sqrt(x)*spiral_freq), np.sin(2*np.pi*np.sqrt(x)*spiral_freq)], -1)\n",
        "u_logvars = np.zeros_like(u_mus) + constant_variance_offset\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "\n",
        "########################################\n",
        "###### Bloated spiral\n",
        "\n",
        "u_mus = u_mus_all[0].copy()\n",
        "u_logvars = u_logvars_all[0].copy() + 2\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "\n",
        "###### Bloated bloated spiral\n",
        "\n",
        "u_mus = u_mus_all[0].copy()\n",
        "u_logvars = u_logvars_all[0].copy()\n",
        "u_logvars = u_logvars + np.sqrt(x).reshape([-1, 1])-4.5\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "###### Square spiral\n",
        "\n",
        "position = np.zeros(2)\n",
        "u_mus = [position.copy()]\n",
        "step_size = 0.5\n",
        "movements = np.float32([[0, 1],\n",
        "                        [1, 0],\n",
        "                        [0, -1],\n",
        "                        [-1, 0]])\n",
        "\n",
        "movement_ind = 0\n",
        "step_ind = 0\n",
        "side_length = 1\n",
        "for i in range(N-1):\n",
        "  position += step_size*movements[movement_ind]\n",
        "  step_ind += 1\n",
        "  if step_ind == side_length:\n",
        "    step_ind = 0\n",
        "    movement_ind = (movement_ind+1) % 4\n",
        "    step_size += 0.1\n",
        "    if not(movement_ind % 2):\n",
        "      side_length += 1\n",
        "  u_mus.append(position.copy())\n",
        "u_mus = np.array(u_mus)\n",
        "\n",
        "u_logvars = np.zeros_like(u_mus)\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "\n",
        "########### more variance\n",
        "position = np.zeros(2)\n",
        "u_mus = [position.copy()]\n",
        "step_size = 0.5\n",
        "movements = np.float32([[0, 1],\n",
        "                        [1, 0],\n",
        "                        [0, -1],\n",
        "                        [-1, 0]])\n",
        "\n",
        "movement_ind = 0\n",
        "step_ind = 0\n",
        "side_length = 1\n",
        "for i in range(N-1):\n",
        "  position += step_size*movements[movement_ind]\n",
        "  step_ind += 1\n",
        "  if step_ind == side_length:\n",
        "    step_ind = 0\n",
        "    movement_ind = (movement_ind+1) % 4\n",
        "    step_size += 0.1\n",
        "    if not(movement_ind % 2):\n",
        "      side_length += 1\n",
        "  u_mus.append(position.copy())\n",
        "u_mus = np.array(u_mus)\n",
        "u_logvars = np.zeros_like(u_mus) + 2.5\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "###### 1D line\n",
        "\n",
        "u_mus = np.linspace(-N/2, N/2, N).reshape([-1, 1])\n",
        "u_logvars = np.zeros_like(u_mus)-1.\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "###### Discrete: two\n",
        "\n",
        "u_mus = np.concatenate([np.ones((N//2, 2))*[[-sqrt_N*0.8, 0]],\n",
        "                            np.ones((N//2, 2))*[[sqrt_N*0.8, 0]]], 0)\n",
        "u_mus = u_mus + np.random.randn(N, 2)*discrete_jitter_level\n",
        "\n",
        "u_logvars = np.zeros_like(u_mus)+1\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "###### Discrete: four\n",
        "\n",
        "u_mus = np.concatenate([\n",
        "    np.ones((N//4, 2))*[[-sqrt_N*0.7, sqrt_N*0.7]],\n",
        "    np.ones((N//4, 2))*[[sqrt_N*0.7, sqrt_N*0.7]],\n",
        "    np.ones((N//4, 2))*[[sqrt_N*0.7, -sqrt_N*0.7]],\n",
        "    np.ones((N//4, 2))*[[-sqrt_N*0.7, -sqrt_N*0.7]]\n",
        "    ], 0)\n",
        "\n",
        "u_mus = u_mus + np.random.randn(N, 2)*0\n",
        "u_logvars = np.zeros_like(u_mus)+1\n",
        "\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "###### Quasi-discrete: four with overlap\n",
        "\n",
        "u_mus = np.concatenate([\n",
        "    np.ones((N//4, 2))*[[-sqrt_N*0.7, sqrt_N*0.7]],\n",
        "    np.ones((N//4, 2))*[[-sqrt_N*0.7, -sqrt_N*0.7]],\n",
        "    np.ones((N//4, 2))*[[sqrt_N*0.7, -sqrt_N*0.7]],\n",
        "    np.ones((N//4, 2))*[[sqrt_N*0.7, sqrt_N*0.7]]\n",
        "    ], 0)\n",
        "\n",
        "vert_variance = 3.\n",
        "\n",
        "u_mus = u_mus + np.random.randn(N, 2)*0\n",
        "u_logvars = np.ones((N, 2))*[[0, vert_variance]]+1\n",
        "u_mus_all.append(u_mus)\n",
        "u_logvars_all.append(u_logvars)\n",
        "\n",
        "## Display\n",
        "plt.figure(figsize=(8, 8))\n",
        "for plt_ind, (mus, logvars) in enumerate(zip(u_mus_all, u_logvars_all)):\n",
        "  plt.subplot(3, 3, plt_ind+1)\n",
        "  if mus.shape[1] == 2:\n",
        "    if plt_ind < 6:\n",
        "      for i in range(N):\n",
        "        ell = Ellipse(xy=mus[i],\n",
        "                      width=2*np.exp(logvars[i, 0]/2.), height=2*np.exp(logvars[i, 1]/2.),\n",
        "                      facecolor=cmap(i/(N-1)), alpha=alpha, edgecolor='k')\n",
        "        plt.gca().add_artist(ell)\n",
        "      plt.ylim(-sqrt_N*1.5, sqrt_N*1.5)\n",
        "      plt.xlim(-sqrt_N*1.5, sqrt_N*1.5)\n",
        "    else:\n",
        "      for i in range(N):\n",
        "        ell = Ellipse(xy=mus[i],\n",
        "                      width=2*np.exp(logvars[i, 0]/2.), height=2*np.exp(logvars[i, 1]/2.),\n",
        "                      facecolor=cmap(i/(N-1)), alpha=1, edgecolor='k')\n",
        "        plt.gca().add_artist(ell)\n",
        "      plt.ylim(-sqrt_N*2, sqrt_N*2)\n",
        "      plt.xlim(-sqrt_N*2, sqrt_N*2)\n",
        "  else:\n",
        "    plt_x = np.linspace(-N/2-20, N/2+20, 10000)\n",
        "    for i in range(20, 40):\n",
        "\n",
        "      sig = np.exp(logvars[i]/2.)\n",
        "      plt_y = np.exp(-np.power((plt_x - mus[i]) / sig, 2.0) / 2) /  (np.sqrt(2.0 * np.pi) * sig)\n",
        "      plt.plot(plt_x, plt_y, lw=4, color=cmap(i/(N-1)))\n",
        "    plt.xlim(-3, 3)\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ROkrccKnBOOK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train an MLP to predict the identity of each datapoint given its embedding; then compute the JSD between the predicted distributions for diff rep spaces\n",
        "model_heads = []\n",
        "head_arch_spec = [256, 256]\n",
        "batch_size = 2**14\n",
        "number_training_steps = 10000\n",
        "learning_rate = 3e-4\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "for latent_space_ind in range(len(u_mus_all)):\n",
        "  print(f'Starting training for latent space {latent_space_ind}.')\n",
        "  ct = time.time()\n",
        "  u_mus = u_mus_all[latent_space_ind]\n",
        "  u_logvars = u_logvars_all[latent_space_ind]\n",
        "  input_dim = u_mus.shape[1]\n",
        "  model_head = tf.keras.Sequential([tf.keras.Input((input_dim,))] + \\\n",
        "   [tf.keras.layers.Dense(number_units, 'leaky_relu') for number_units in head_arch_spec] + \\\n",
        "    [tf.keras.layers.Dense(N, 'softmax')])\n",
        "\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "  @tf.function\n",
        "  def train_step():\n",
        "    batch_inds = tf.random.categorical(tf.zeros((1,N)), batch_size, dtype=tf.int32)[0]\n",
        "    mus = tf.gather(u_mus, batch_inds)\n",
        "    logvars = tf.gather(u_logvars, batch_inds)\n",
        "    reparam = tf.random.normal(tf.shape(mus), mean=mus, stddev=tf.exp(logvars/2.), dtype=tf.float64)\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model_head(reparam)\n",
        "      loss = loss_fn(batch_inds, preds)\n",
        "    grads = tape.gradient(loss, model_head.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model_head.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "  loss_series = []\n",
        "  for _ in range(number_training_steps):\n",
        "    loss_series.append(train_step().numpy())\n",
        "\n",
        "  print(f'Finished training classification head for latent space {roman_numerals[latent_space_ind]}.  Time taken: {time.time()-ct:.3f} sec / {number_training_steps} steps.')\n",
        "\n",
        "  plt.figure(figsize=(6, 4))\n",
        "  plt.plot(loss_series, lw=2)\n",
        "  plt.show()\n",
        "\n",
        "  model_heads.append(model_head)"
      ],
      "metadata": {
        "id": "Wznx7lttKaUO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Calculate the average output distribution for each input x_i, and make a matrix for comparison w Bhattacharyya distinguishability\n",
        "eval_size = 1000\n",
        "for latent_space_ind in range(len(u_mus_all)):\n",
        "  u_mus = u_mus_all[latent_space_ind]\n",
        "  u_logvars = u_logvars_all[latent_space_ind]\n",
        "  model_head = model_heads[latent_space_ind]\n",
        "  avg_dists = []\n",
        "  for input_ind in range(N):\n",
        "    mus = tf.gather(u_mus, tf.ones(eval_size, dtype=tf.int32)*input_ind)\n",
        "    logvars = tf.gather(u_logvars, tf.ones(eval_size, dtype=tf.int32)*input_ind)\n",
        "    reparam = tf.random.normal(tf.shape(mus), mean=mus, stddev=tf.exp(logvars/2.), dtype=tf.float64)\n",
        "\n",
        "    preds = model_head(reparam)\n",
        "\n",
        "    avg_dists.append(tf.reduce_mean(preds, 0))\n",
        "\n",
        "  avg_dists = tf.stack(avg_dists, 0)\n",
        "\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(121)\n",
        "  plt.imshow(avg_dists/np.max(avg_dists, 1, keepdims=True), vmin=0, vmax=1, cmap='Blues_r')\n",
        "  plt.xticks(range(0, 64, 8), range(0, 64, 8))\n",
        "  plt.yticks(range(0, 64, 8), range(0, 64, 8))\n",
        "  plt.title('Average output distribution p(\\hat{x}_j|x_i)')\n",
        "  plt.subplot(122)\n",
        "  plt.imshow(np.exp(-bhattacharyya_dist_mat_tf(u_mus, u_logvars)), vmin=0, vmax=1, cmap='Blues_r')\n",
        "  plt.xticks(range(0, 64, 8), range(0, 64, 8))\n",
        "  plt.yticks(range(0, 64, 8), range(0, 64, 8))\n",
        "  plt.title('Bhattacharyya distinguishability')\n",
        "  plt.suptitle(f'Latent space {roman_numerals[latent_space_ind]}', fontsize=18, y=0.98)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ppnxrmqYmbZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The actual pairwise comparison calculations"
      ],
      "metadata": {
        "id": "Py5piSyQg_vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compute the JSD between the average output distributions for different latent spaces\n",
        "kld = tf.keras.losses.KLDivergence()\n",
        "xent = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "jsds = np.eye((len(u_mus_all)))\n",
        "\n",
        "eval_size = 1000\n",
        "\n",
        "avg_dists_all = []\n",
        "for latent_space_ind in range(len(u_mus_all)):\n",
        "  u_mus = u_mus_all[latent_space_ind]\n",
        "  u_logvars = u_logvars_all[latent_space_ind]\n",
        "  model_head = model_heads[latent_space_ind]\n",
        "  avg_dists = []\n",
        "  for input_ind in range(N):\n",
        "    mus = tf.gather(u_mus, tf.ones(eval_size, dtype=tf.int32)*input_ind)\n",
        "    logvars = tf.gather(u_logvars, tf.ones(eval_size, dtype=tf.int32)*input_ind)\n",
        "    reparam = tf.random.normal(tf.shape(mus), mean=mus, stddev=tf.exp(logvars/2.), dtype=tf.float64)\n",
        "\n",
        "    preds = model_head(reparam)\n",
        "\n",
        "    avg_dists.append(tf.reduce_mean(preds, 0))\n",
        "\n",
        "  avg_dists = tf.stack(avg_dists, 0)\n",
        "  avg_dists_all.append(avg_dists)\n",
        "\n",
        "for latent_space_ind1 in range(len(u_mus_all)):\n",
        "  preds1 = avg_dists_all[latent_space_ind1]\n",
        "  xent1 = xent(tf.range(N), preds1)\n",
        "  print(f'Info in {latent_space_ind1}: {(np.log(N)-xent1).numpy()/np.log(2):.2f} bits')\n",
        "\n",
        "  for latent_space_ind2 in range(latent_space_ind1, len(u_mus_all)):\n",
        "    preds2 = avg_dists_all[latent_space_ind2]\n",
        "    averaged_preds = (preds1+preds2)/2.\n",
        "\n",
        "    jsd = (kld(preds1, averaged_preds) + kld(preds2, averaged_preds))/2.\n",
        "\n",
        "    jsds[latent_space_ind1, latent_space_ind2] = jsd\n",
        "    jsds[latent_space_ind2, latent_space_ind1] = jsd"
      ],
      "metadata": {
        "id": "pDe-S2FJ_vUW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Measure all the pairwise similarities in latent space\n",
        "stoch_shape_metric_scores = np.eye(len(u_mus_all))\n",
        "nmis_bhat, vis_bhat = [np.eye(len(u_mus_all)), np.eye(len(u_mus_all))]\n",
        "cka_bhat, cka_linear = [np.eye(len(u_mus_all)), np.eye(len(u_mus_all))]\n",
        "nmis_mc, vis_mc = [np.eye(len(u_mus_all)), np.eye(len(u_mus_all))]\n",
        "cka_nonlinear, rsa_nonlinear = [np.eye(len(u_mus_all)), np.eye(len(u_mus_all))]\n",
        "infos_mc_single, infos_mc_double, infos_mc_combo = [[], [], []]\n",
        "number_mc_random_samples = 5\n",
        "fractional_infos = []\n",
        "\n",
        "for embedding_space_ind1 in range(len(u_mus_all)):\n",
        "  u_mus = u_mus_all[embedding_space_ind1]\n",
        "  u_logvars = u_logvars_all[embedding_space_ind1]\n",
        "  i1 = monte_carlo_info(u_mus, u_logvars, number_random_samples=number_mc_random_samples)\n",
        "  i11 = monte_carlo_info(np.tile(u_mus, [1, 2]),\n",
        "                        np.tile(u_logvars, [1, 2]), number_random_samples=number_mc_random_samples)\n",
        "  fractional_infos.append(i1/np.log2(N))\n",
        "  for embedding_space_ind2 in range(embedding_space_ind1, len(u_mus_all)):\n",
        "\n",
        "    v_mus = u_mus_all[embedding_space_ind2]\n",
        "    v_logvars = u_logvars_all[embedding_space_ind2]\n",
        "    if u_mus.shape[1] == v_mus.shape[1]:\n",
        "      ssm = compute_stoch_shape_metric(u_mus, u_logvars, v_mus, v_logvars)\n",
        "    else:\n",
        "      ssm = np.nan\n",
        "    stoch_shape_metric_scores[embedding_space_ind1, embedding_space_ind2] = ssm\n",
        "    stoch_shape_metric_scores[embedding_space_ind2, embedding_space_ind1] = ssm\n",
        "\n",
        "\n",
        "    i2 = monte_carlo_info(v_mus, v_logvars, number_random_samples=number_mc_random_samples)\n",
        "    i22 = monte_carlo_info(np.tile(v_mus, [1, 2]),\n",
        "                          np.tile(v_logvars, [1, 2]), number_random_samples=number_mc_random_samples)\n",
        "    i12 = monte_carlo_info(np.concatenate([u_mus, v_mus], 1),\n",
        "                          np.concatenate([u_logvars, v_logvars], 1), number_random_samples=number_mc_random_samples)\n",
        "\n",
        "    infos_mc_single.append([i1, i2])\n",
        "    infos_mc_double.append([i11, i22])\n",
        "    infos_mc_combo.append([i12])\n",
        "    nmi = (i1+i2-i12) / tf.sqrt((2*i1-i11)*(2*i2-i22))\n",
        "    nmis_mc[embedding_space_ind1, embedding_space_ind2] = nmi\n",
        "    nmis_mc[embedding_space_ind2, embedding_space_ind1] = nmi\n",
        "    vi = 2*i12 - i11 - i22\n",
        "    vis_mc[embedding_space_ind1, embedding_space_ind2] = vi\n",
        "    vis_mc[embedding_space_ind2, embedding_space_ind1] = vi\n",
        "\n",
        "    bhat1 = bhattacharyya_dist_mat_tf(u_mus, u_logvars)\n",
        "    bhat2 = bhattacharyya_dist_mat_tf(v_mus, v_logvars)\n",
        "\n",
        "    nmi = compute_nmi_bhat_tf(bhat1, bhat2)\n",
        "    nmis_bhat[embedding_space_ind1, embedding_space_ind2] = nmi\n",
        "    nmis_bhat[embedding_space_ind2, embedding_space_ind1] = nmi\n",
        "\n",
        "    vi = compute_vi_bhat_tf(bhat1, bhat2)\n",
        "    vis_bhat[embedding_space_ind1, embedding_space_ind2] = vi\n",
        "    vis_bhat[embedding_space_ind2, embedding_space_ind1] = vi\n",
        "\n",
        "    cka = compute_cka_tf(tf.exp(-bhat1), tf.exp(-bhat2))\n",
        "    cka_bhat[embedding_space_ind1, embedding_space_ind2] = cka\n",
        "    cka_bhat[embedding_space_ind2, embedding_space_ind1] = cka\n",
        "\n",
        "    dotp1 = tf.matmul(u_mus, u_mus, transpose_b=True)\n",
        "    dotp2 = tf.matmul(v_mus, v_mus, transpose_b=True)\n",
        "    cka = compute_cka_tf(dotp1, dotp2)\n",
        "    cka_linear[embedding_space_ind1, embedding_space_ind2] = cka\n",
        "    cka_linear[embedding_space_ind2, embedding_space_ind1] = cka\n",
        "\n",
        "    dist_sq1 = tf.exp(-pairwise_l2_distance(u_mus, u_mus) / tf.exp(tf.reduce_mean(u_logvars)) / 2)\n",
        "    dist_sq2 = tf.exp(-pairwise_l2_distance(v_mus, v_mus) / tf.exp(tf.reduce_mean(v_logvars)) / 2)\n",
        "    l1 = tf.reduce_mean(tf.exp(u_logvars/2.))\n",
        "    l2 = tf.reduce_mean(tf.exp(v_logvars/2.))\n",
        "    dist_sq1 = tf.exp(-pairwise_l2_distance(u_mus, u_mus) / l1**2 / 2)\n",
        "    dist_sq2 = tf.exp(-pairwise_l2_distance(v_mus, v_mus) / l2**2 / 2)\n",
        "    cka = compute_cka_tf(dist_sq1, dist_sq2)\n",
        "    cka_nonlinear[embedding_space_ind1, embedding_space_ind2] = cka\n",
        "    cka_nonlinear[embedding_space_ind2, embedding_space_ind1] = cka"
      ],
      "metadata": {
        "id": "Fw7ftakF8a6M",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aesthetic_index_reorder = [7, 0, 1, 8, 4, 5, 6, 2, 3]\n",
        "\n",
        "labels = ['cka_lin', 'cka_bhat', 'nmi_mc', 'nmi_bhat', 'stoch_shape', 'vi_mc', 'vi_bhat', 'jsd', 'cka_non']\n",
        "similarities = [cka_linear, cka_bhat, nmis_mc, nmis_bhat, stoch_shape_metric_scores,  vis_mc, vis_bhat, jsds, cka_nonlinear]\n",
        "cmaps = ['magma', 'magma', 'magma', 'magma', 'magma_r', 'magma_r', 'magma_r', 'magma_r', 'magma']\n",
        "vmin = 0\n",
        "vmaxes = [1, 1, 1, 1, None,  3.2, 2.3, None, 1]\n",
        "plt.figure(figsize=(32, 10))\n",
        "for plt_ind, (similarity_values, label, cmap, vmax) in enumerate(zip(similarities, labels, cmaps, vmaxes)):\n",
        "  plt.subplot(2, 5, aesthetic_index_reorder.index(plt_ind)+1)\n",
        "\n",
        "  plt.imshow(np.reshape(similarity_values, [len(u_mus_all), -1]), vmin=vmin, vmax=vmax, cmap=cmap)\n",
        "  plt.colorbar()\n",
        "  plt.title(label, fontsize=15)\n",
        "  plt.xticks(np.arange(len(u_mus_all)), roman_numerals[:len(u_mus_all)])\n",
        "  plt.yticks(np.arange(len(u_mus_all)), roman_numerals[:len(u_mus_all)])\n",
        "  plt.tick_params(axis='both', which='both',length=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HQAtflC2uBvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spearmans = np.zeros((len(similarities), len(similarities)))*np.nan\n",
        "plt.figure(figsize=(6, 6))\n",
        "for sim_ind1 in range(len(similarities)):\n",
        "  for sim_ind2 in range(sim_ind1+1, len(similarities)):\n",
        "    sim1 = similarities[aesthetic_index_reorder[sim_ind1]][np.triu_indices(similarities[0].shape[0], k=1)]\n",
        "    sim2 = similarities[aesthetic_index_reorder[sim_ind2]][np.triu_indices(similarities[0].shape[0], k=1)]\n",
        "    if np.any(np.isnan(sim1)):\n",
        "      incl_inds = np.logical_not(np.isnan(sim1))\n",
        "      sim1 = sim1[incl_inds]\n",
        "      sim2 = sim2[incl_inds]\n",
        "    elif np.any(np.isnan(sim2)):\n",
        "      incl_inds = np.logical_not(np.isnan(sim2))\n",
        "      sim1 = sim1[incl_inds]\n",
        "      sim2 = sim2[incl_inds]\n",
        "\n",
        "\n",
        "    spearmans[sim_ind1, sim_ind2] = scipy.stats.spearmanr(sim1, sim2)[0]\n",
        "    plt.text(sim_ind2, sim_ind1, f'{spearmans[sim_ind1, sim_ind2]:.2f}', va='center', ha='center')\n",
        "plt.imshow(np.abs(spearmans), vmin=None, vmax=1, cmap='Reds')\n",
        "\n",
        "plt.xticks(range(len(similarities)), [labels[i] for i in aesthetic_index_reorder], rotation=90)\n",
        "plt.yticks(range(len(similarities)), [labels[i] for i in aesthetic_index_reorder])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lyz4uu3z0TUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 18))\n",
        "for sim_ind1 in range(len(similarities)):\n",
        "  for sim_ind2 in range(sim_ind1, len(similarities)):\n",
        "    plt.subplot(len(similarities), len(similarities), sim_ind1*len(similarities)+sim_ind2+1)\n",
        "    if sim_ind1 == sim_ind2:\n",
        "      sims = similarities[aesthetic_index_reorder[sim_ind1]]\n",
        "      sims = sims[np.logical_not(np.isnan(sims))]\n",
        "      hist, bins = np.histogram(sims, 10)\n",
        "      plt.bar(bins[:-1], hist, width=np.diff(bins), alpha=0.5, color=default_colors[0])\n",
        "      plt.ylabel(labels[aesthetic_index_reorder[sim_ind1]], fontsize=16)\n",
        "    else:\n",
        "      plt.scatter(similarities[aesthetic_index_reorder[sim_ind1]].flatten(), similarities[aesthetic_index_reorder[sim_ind2]].flatten(), s=3, c='k')\n",
        "    if sim_ind1 == 0:\n",
        "      plt.title(labels[aesthetic_index_reorder[sim_ind2]], fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hkqdM2aOxzrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IJgE60FjgPsH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}